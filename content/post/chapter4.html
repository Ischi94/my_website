---
title: "Rethinking Chapter 4"
author: "Gregor Mathes"
date: "2021-01-01"
slug: Rethinking Chapter 4
categories: []
tags: [Rethinking, Bayes, Statistics]
subtitle: ''
summary: 'This is the third part of a series where I work through the practice questions of the second edition of Richard McElreaths Statistical Rethinking'
authors: [Gregor Mathes]
lastmod: '2021-01-01T12:07:04+02:00'
featured: no
projects: [Rethinking]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_width: 6
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#homework"><span class="toc-section-number">2</span> Homework</a></li>
<li><a href="#easy-practices"><span class="toc-section-number">3</span> Easy practices</a></li>
<li><a href="#medium-practices"><span class="toc-section-number">4</span> Medium practices</a></li>
<li><a href="#hard-practices"><span class="toc-section-number">5</span> Hard practices</a></li>
</ul>
</div>

<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This is the third part of a series where I work through the practice questions of the second edition of Richard McElreaths <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>. Each post covers a new chapter. There are already some awesome sources for this book online like <a href="https://jmgirard.com/statistical-rethinking-ch2/">Jeffrey Girard</a> working through the exercises of the first edition, or <a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/">Solomon Kurz</a> leading through each example of the book with the <em>brms</em> and the <em>tidyverse</em> packages. You can even watch the <a href="https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI">lectures of McElreath</a> on Youtube and work through the <a href="https://github.com/rmcelreath/statrethinking_winter2019/tree/master/homework">homework and solutions</a>.
However, so far I couldn’t find a source providing solutions for the practice questions of the second edition, or the homework practices, in a tidy(-verse) way. My aim here is therefore to provide solutions for each homework and practice question of the second edition, using the <em>tidyverse</em> and the <em>rethinking</em> packages. The third part of the series will cover chapter 4, which corresponds to week 2 of the lectures and homework.<br />
McElreath himself states that chapter 4 provides the foundation for most of the examples of the later chapters. So I will spend a bit more time on this chapter and go a bit more into detail.</p>
</div>
<div id="homework" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Homework</h1>
<div id="question-1" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Question 1</h2>
<p><strong>The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions.</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">Individual</th>
<th align="center">weight</th>
<th align="right">expected height</th>
<th align="right">89% Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">46.95</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">43.72</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">64.78</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">32.59</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">54.63</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>We can use a linear regression model to predict height from weight. First, let’s load the census data and the new weights:</p>
<pre class="r"><code># data
data(&quot;Howell1&quot;)
d &lt;- Howell1

# new data to predict from 
new_weight &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)</code></pre>
<p>For the model, we use the same structur and priors as given on page 102. Further, we use all data (both juveniles and adults) and use quadratic approximation for the posterior.</p>
<pre class="r"><code># define the average weight
xbar &lt;- d %&gt;% summarise(xbar = mean(weight)) %&gt;% pull

# model formula
m &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * (weight - xbar),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)) %&gt;% 
  # model fit using quadratic approximation
  quap(data = d)</code></pre>
<p>Now we can calculate the posterior distribution of heights for each individual weight value in the table using the <code>link()</code> function, as explained on page 105. From these posterior distributions, we can calculate the mean and the 89% percentile interval using <code>summarise_all()</code>.</p>
<pre class="r"><code># predict height 
pred_height &lt;- link(m, data = data.frame(weight = new_weight))

# calculate means
expected &lt;- pred_height %&gt;% 
  as_tibble() %&gt;% 
  summarise_all(mean) %&gt;% 
  as_vector()

# calculate percentile interval
interval &lt;- pred_height %&gt;% 
  as_tibble() %&gt;% 
  summarise_all(HPDI, prob = 0.89) %&gt;% 
  as_vector()</code></pre>
<p>Now we just have to add the predicted values to the table:</p>
<pre class="r"><code>table_height &lt;- tibble(individual = 1:5, weight = new_weight, expected = expected,
                       lower = interval[c(TRUE, FALSE)], upper = interval[c(FALSE, TRUE)]) %&gt;% 
  knitr::kable(align = &quot;cccrr&quot;)

table_height</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">individual</th>
<th align="center">weight</th>
<th align="center">expected</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">46.95</td>
<td align="center">158.2962</td>
<td align="right">157.4396</td>
<td align="right">159.1011</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">43.72</td>
<td align="center">152.5978</td>
<td align="right">151.8108</td>
<td align="right">153.3172</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">64.78</td>
<td align="center">189.7522</td>
<td align="right">188.3641</td>
<td align="right">191.2205</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">32.59</td>
<td align="center">132.9621</td>
<td align="right">132.3597</td>
<td align="right">133.6208</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">54.63</td>
<td align="center">171.8454</td>
<td align="right">170.9054</td>
<td align="right">173.0321</td>
</tr>
</tbody>
</table>
</div>
<div id="question-2" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Question 2</h2>
<p><strong>Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire <code>Howell1</code> data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation. Use any model type from chapter 4 that you think useful: an ordinary linear regression, a polynomial or a spline. Plot the posterior predictions against the raw data.</strong></p>
<p>First, let’s take a look at the data:</p>
<pre class="r"><code>d %&gt;% 
  mutate(log.weight = log(weight)) %&gt;% 
  ggplot() +
  geom_point(aes(log.weight, height), alpha = 0.5) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%202%20part%201-1.png" width="672" /></p>
<p>It actually looks like a decent linear relationship, so a simple linear regression should be sufficient. All we need to change from the previous model is to log-transform the weight.</p>
<pre class="r"><code>m_log &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * log(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)) %&gt;% 
  quap(data = d)</code></pre>
<p>Let’s glimpse at the results:</p>
<pre class="r"><code>precis(m_log) %&gt;% as_tibble() %&gt;% 
  add_column(parameter = rownames(precis(m_log))) %&gt;% 
  rename(&quot;lower&quot; = &#39;5.5%&#39;, &quot;upper&quot; = &#39;94.5%&#39;) %&gt;% 
  select(parameter, everything()) %&gt;% 
  knitr::kable(align = &quot;lcrrr&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="center">mean</th>
<th align="right">sd</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="center">-22.874316</td>
<td align="right">1.3342912</td>
<td align="right">-25.006771</td>
<td align="right">-20.741861</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="center">46.817787</td>
<td align="right">0.3823241</td>
<td align="right">46.206759</td>
<td align="right">47.428815</td>
</tr>
<tr class="odd">
<td align="left">sigma</td>
<td align="center">5.137088</td>
<td align="right">0.1558847</td>
<td align="right">4.887954</td>
<td align="right">5.386222</td>
</tr>
</tbody>
</table>
<p>Instead of trying to read these estimates, we can just visualise our model. Let’s calculate the predicted mean height as a function of weight, the 97% PI for the mean, and the 97% PI for predicted heights as explained on page 108.</p>
<p>As we will repeat these steps throughout the exercises, we can set up a function for the interval calculation:</p>
<pre class="r"><code># we better make a function out of it since we use it more often
tidy_intervals &lt;- function(my_function, my_model, interval_type, 
                           x_var, x_seq){
  # preprocess dataframe
  df &lt;- data.frame(col1 = x_seq)
  colnames(df) &lt;- x_var
  
  # calculate 89% intervals for each weight
  # either link or sim
  my_function(my_model, data = df) %&gt;% 
  as_tibble() %&gt;% 
  #  either PI or HPDI
  summarise_all(interval_type, prob = 0.89) %&gt;% 
  add_column(type = c(&quot;lower&quot;, &quot;upper&quot;)) %&gt;% 
  pivot_longer(cols = -type, names_to = &quot;cols&quot;, values_to = &quot;intervals&quot;) %&gt;% 
  add_column(x_var = rep(x_seq, 2)) %&gt;% 
  pivot_wider(names_from = type, values_from = intervals)
}</code></pre>
<p>And for the mean:</p>
<pre class="r"><code>tidy_mean &lt;- function(my_model, data){
  link(my_model, data) %&gt;%
    as_tibble() %&gt;%
    summarise_all(mean) %&gt;%
    pivot_longer(cols = everything()) %&gt;% 
    add_column(x_var = data[[1]])
}</code></pre>
<p>Now let’s use these functions to calculate the intervals:</p>
<pre class="r"><code># define weight range
weight_seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)

# calculate 89% intervals for each weight
intervals &lt;- tidy_intervals(link, m_log, HPDI, 
                            x_var = &quot;weight&quot;, x_seq = weight_seq)

# calculate mean
reg_line &lt;- tidy_mean(m_log, data = data.frame(weight = weight_seq))


# calculate prediction intervals
pred_intervals &lt;- tidy_intervals(sim, m_log, PI, 
                                 x_var = &quot;weight&quot;, x_seq = weight_seq)</code></pre>
<p>Now we can plot the raw data, the posterior mean from, the distribution of mu (which corresponds to the 89% HPDI of the mean), and the region within which the model expects to find 89% of actual heights in the population.</p>
<pre class="r"><code>ggplot(d) +
  geom_point(aes(weight, height), alpha = 0.5) +
  geom_ribbon(aes(x = x_var, ymin = lower, ymax = upper),
                alpha=0.8, data = intervals) +
  geom_ribbon(aes(x = x_var, ymin = lower, ymax = upper),
                alpha=0.2, data = pred_intervals) +
  geom_line(aes(x = x_var, y = value), data = reg_line, colour = &quot;coral&quot;) +
  theme_light()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%202%20part%207-1.png" width="672" /></p>
<p>This looks like a decent fit. Let’s make a function of the ggplot, so we can call it later on:</p>
<pre class="r"><code>plot_regression &lt;- function(df, x, y, 
                            interv = intervals, pred_interv = pred_intervals){
  ggplot(df) +
    geom_point(aes({{x}}, {{y}}), alpha = 0.5) +
    geom_ribbon(aes(x = x_var, ymin = lower, ymax = upper),
                alpha=0.8, data = interv) +
    geom_ribbon(aes(x = x_var, ymin = lower, ymax = upper),
                alpha=0.2, data = pred_interv) +
    geom_line(aes(x = x_var, y = value), data = reg_line, colour = &quot;coral&quot;) +
    theme_light()
}</code></pre>
</div>
<div id="question-3" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Question 3</h2>
<p><strong>Plot the prior predictive distribution for the polynomial regression model in Chapter 4. You can modify the prior predictive distribution. 20 or 30 parabolas from the prior should suffice to show where the prior probability resides. Can you modify the prior distributions of a, b1, and b2 so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data.</strong></p>
<p>Let us first recap how the polynomial regression model in Chapter 4 was built: The first thing was to standardise the predictor variables, in our case the <code>weight</code> variable. Additionally, we calculate the square of the standardised weight.</p>
<pre class="r"><code># standardise weight
d_stand &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight)) / sd(weight), 
                        weight_s2 = weight_s^2) %&gt;% 
  as_tibble()</code></pre>
<p>And here’s the notation of the model with the priors used in the chapter:</p>
<pre class="r"><code># fit model
m_poly &lt;- alist(height ~ dnorm(mu, sigma), 
               mu &lt;- a + b1 * weight_s + b2 * weight_s2,
               a ~ dnorm(178, 20), 
               b1 ~ dlnorm(0, 1), 
               b2 ~ dnorm(0, 1), 
               sigma ~ dunif(0, 50)) %&gt;% 
  quap(data = d_stand)</code></pre>
<p>To get the prior prediction, we can sample from the prior distribution using the <code>extract.prior()</code> function. We then pass these samples from the prior to the link function for the weight space we are interested in. As we want to try out various priors and how they affect the prediction, I’ll make a function that takes a model definition (an <code>alist</code>) and the number of predicted curves as argument.</p>
<pre class="r"><code>modify_prior_poly &lt;- function(my_alist, N) {
  
  # set seed for reproducibility
  set.seed(0709)
  
  # fit model
  m_poly &lt;- my_alist %&gt;%
    quap(data = d_stand)
  
  
  # make weight sequence with both standardised weight and the square of it
  weight_seq &lt;- tibble(weight = seq(
    from = min(d_stand$weight),
    to =  max(d_stand$weight),
    by = 1
  )) %&gt;%
    mutate(weight_s = (weight - mean(weight)) / sd(weight),
           weight_s2 = weight_s ^ 2)
  
  # extract samples from the prior
  m_poly_prior &lt;- extract.prior(m_poly, n = N)
  
  # now apply the polynomial equation to the priors to get predicted heights
  m_poly_mu &lt;- link(
    m_poly,
    post = m_poly_prior,
    data = list(
      weight_s = weight_seq$weight_s,
      weight_s2 = weight_seq$weight_s2
    )
  ) %&gt;%
    as_tibble() %&gt;%
    pivot_longer(cols = everything(), values_to = &quot;height&quot;) %&gt;%
    add_column(
      weight = rep(weight_seq$weight, N),
      type = rep(as.character(1:N), each = length(weight_seq$weight))
    )
  
  # plot it 
  ggplot(m_poly_mu) +
    geom_line(aes(x = weight, y = height, group = type), alpha = 0.5) +
    geom_hline(yintercept = c(0, 272), colour = &quot;steelblue4&quot;) +
    annotate(
      geom = &quot;text&quot;,
      x = c(6, 12),
      y = c(11, 285),
      label = c(&quot;Embryo&quot;, &quot;World&#39;s tallest person&quot;),
      colour = c(rep(&quot;steelblue4&quot;, 2))
    ) +
    labs(x = &quot;Weight in kg&quot;, y = &quot;Height in cm&quot;) +
    theme_minimal()
}</code></pre>
<p>We can now define our own model via an <code>alist</code> and then throw it into our function, and directly get the (visualised) output. Let’s start with the priors used in the chapter, with 40 predictive curves sampled from the priors.</p>
<pre class="r"><code>alist(height ~ dnorm(mu, sigma), 
      mu &lt;- a + b1 * weight_s + b2 * weight_s2,
      a ~ dnorm(178, 20), 
      b1 ~ dlnorm(0, 1), 
      b2 ~ dnorm(0, 1), 
      sigma ~ dunif(0, 50)) %&gt;% 
  modify_prior_poly(my_alist = ., N = 40)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%203%20part%204-1.png" width="672" /></p>
<p>It seems like our priors are a bit too narrow as we do not cover the whole potential/ realistic space. So what we can do is decrease the prior on the mean of alpha a bit, while increasing its standard deviation:</p>
<pre class="r"><code>alist(height ~ dnorm(mu, sigma), 
      mu &lt;- a + b1 * weight_s + b2 * weight_s2,
      a ~ dnorm(130, 35), # decrease mean and increase sd
      b1 ~ dlnorm(0, 1), 
      b2 ~ dnorm(0, 1), 
      sigma ~ dunif(0, 50)) %&gt;% 
  modify_prior_poly(my_alist = ., N = 40)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%203%20part%205-1.png" width="672" /></p>
<p>Now that looks more realistic to me, without knowing anything about the data. The prior on alpha now looks good to me, as we are expressing enough uncertainty now. The trend lines hower are a bit too straight (can you say that?) basically assuming no relationship between height and weight. But we already know, without looking at the data, that there is a positive relationship. This is prior knowledge we can and should use. So let’s try making the slope of our prior predictive lines a bit more positive. We can do so by adjusting the prior on beta1. Now to tune the prior for the log-normal distribution is quite difficult. If we decrease the standard deviation, our distribution gets less skewed, meaning that we get less values that are very high but also less values that are very low. Instead, we will directly change the mean of the log-normal. When we increase it, we increase the chance of getting higher values for the slope:</p>
<pre class="r"><code>alist(height ~ dnorm(mu, sigma), 
      mu &lt;- a + b1 * weight_s + b2 * weight_s2,
      a ~ dnorm(130, 35), 
      b1 ~ dlnorm(1, 1), # increase mean 
      b2 ~ dnorm(0, 1), 
      sigma ~ dunif(0, 50)) %&gt;% 
  modify_prior_poly(my_alist = ., N = 40)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%203%20part%206-1.png" width="672" /></p>
<p>This looks better now. We are not too certain about the relationship, but at least we directly included our prior knowledge now. One last thing we could change is the prior on beta2. This prior affects the curvature of the line. But, taking into account what we know about the relationship, I am quite happy about the curvature. We could just force it to be positive by using a narrow log-normal distribution, because a negative value will result in a downwards curvature (= negative relationship).</p>
<pre class="r"><code>alist(height ~ dnorm(mu, sigma), 
      mu &lt;- a + b1 * weight_s + b2 * weight_s2,
      a ~ dnorm(130, 35), 
      b1 ~ dlnorm(1, 1),  
      b2 ~ dlnorm(0, 0.5), # force positivity
      sigma ~ dunif(0, 50)) %&gt;% 
  modify_prior_poly(my_alist = ., N = 40)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%203%20part%207-1.png" width="672" /></p>
<p>That’s it. This is my first exercise fuzzing with priors and it really forced me to <em>think</em> about my model assumption. This takes a lot of time, but will eventually result in better models.</p>
</div>
</div>
<div id="easy-practices" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Easy practices</h1>
<div id="question-4e1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Question 4E1</h2>
<p><strong>In the model definition below, which line is the likelihood?</strong></p>
<p><span class="math inline">\(y_{i} ∼ Normal(μ,σ)\)</span><br />
<span class="math inline">\(μ ∼ Normal(0,10)\)</span><br />
<span class="math inline">\(σ ∼ Exponential(1)\)</span></p>
<p>We can follow the example on page 82: The first line is therefore the likelihood, the second line the prior on the mean, and the third the prior on the standard deviation.</p>
</div>
<div id="question-4e2" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Question 4E2</h2>
<p><strong>In the model definition just above, how many parameters are in the posterior distribution?</strong></p>
<p>Y is not a parameter. It is the observed data (page 82). Hence, there are two parameters to be estimated in this model: μ and σ.</p>
</div>
<div id="question-4e3" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Question 4E3</h2>
<p><strong>Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors</strong></p>
<p>We can simply follow the example on page 83:<br />
<span class="math inline">\(Pr(μ,σ|y)=∏iNormal(yi|μ,σ)Normal(μ|0,10)Uniform(σ|0,10)/∫∫∏iNormal(hi|μ,σ)Normal(μ|0,10)Uniform(σ|0,10)dμdσ\)</span></p>
<p>Question 4E4</p>
<p><strong>In the model definition below, which line is the linear model?</strong></p>
<p><span class="math inline">\(y_{i} ∼ Normal(μ,σ)\)</span><br />
<span class="math inline">\(μ_{i} = α + βx_{i}\)</span><br />
<span class="math inline">\(α ~ Normal(0, 10)\)</span><br />
<span class="math inline">\(β ∼ Normal(0,1)\)</span><br />
<span class="math inline">\(σ ∼ Exponential(2)\)</span></p>
<p>Following the example on page 77: The first line is the likelihood. The second the linear model. The third the prior on α. The fourth the prior on β. And the fifth the prior on σ.</p>
</div>
</div>
<div id="medium-practices" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Medium practices</h1>
<div id="question-4m1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Question 4M1</h2>
<p><strong>For the model definition below, simulate observed y values from the prior (not the posterior).</strong></p>
<p><span class="math inline">\(y_{i} ∼ Normal(μ,σ)\)</span><br />
<span class="math inline">\(μ ∼ Normal(0,10)\)</span><br />
<span class="math inline">\(σ ∼ Exponential(1)\)</span></p>
<p>We can simply sample from each prior distribution:</p>
<pre class="r"><code># sample mu
sample_mu &lt;- rnorm(1e4, 0, 10)

# sample sigma
sample_sigma &lt;- rexp(1e4, 1)

# sample y
prior_y &lt;- rnorm(1e4, sample_mu, sample_sigma) %&gt;% 
  enframe()

# plot
ggplot(prior_y) +
  geom_density(aes(value), size = 1) + 
  theme_light()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M1-1.png" width="672" /></p>
</div>
<div id="question-4m2" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Question 4M2</h2>
<p><strong>Translate the model just above into a <code>quap()</code> formula</strong></p>
<pre class="r"><code>quap_frm &lt;- alist(y ~ dnorm(mu, sigma), 
                 mu ~ dnorm(0, 10), 
                 sigma ~ dexp(1)) </code></pre>
</div>
<div id="question-4m3" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Question 4M3</h2>
<p><strong>Translate the <code>quap()</code> model formula below into a mathematical model definition.</strong></p>
<pre class="r"><code>flist &lt;- alist(
  y ~ dnorm(mu, sigma),
  mu &lt;- a + b*x,
  a ~ dnorm(0, 50),
  b ~ dunif(0, 10),
  sigma ~ dunif(0, 50))</code></pre>
<p><span class="math inline">\(y_{1} ∼ Normal(μ,σ)\)</span><br />
<span class="math inline">\(μ_{1} ∼ α + βx_{1}\)</span><br />
<span class="math inline">\(α ∼ Normal(0, 50)\)</span><br />
<span class="math inline">\(β = Uniform(0, 10)\)</span><br />
<span class="math inline">\(σ ∼ Uniform(0, 50)\)</span></p>
</div>
<div id="question-4m4" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Question 4M4</h2>
<p><strong>A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.</strong></p>
<p>We can use a simple linear regression approach for this.<br />
First the likelihood, assuming that height is normally distributed:<br />
<span class="math inline">\(h_{1} ∼ Normal(μ,σ)\)</span></p>
<p>The linear model:<br />
<span class="math inline">\(μ_{1} ∼ α + βx_{1}\)</span></p>
<p>The prior for the intercept. It says nothing about the age of the students, so I use a weak prior for the intercept, covering elementary school to adults as shown in the plot below:<br />
<span class="math inline">\(α ∼ Normal(150, 20)\)</span></p>
<pre class="r"><code>tibble(height = rnorm(10000, 150, 20)) %&gt;% 
  ggplot() +
  geom_density(aes(height)) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M4%20part%201-1.png" width="672" /></p>
<p>Likewise, the prior for the growth rate needs to adjust for fast growing juveniles and for smaller growing older students. We can’t use a normal distribution because students don’t shrink (to my knowledge). A uniform distribution forces only positive growth rates.<br />
<span class="math inline">\(β = Uniform(0, 7)\)</span></p>
<pre class="r"><code>tibble(growth = runif(10000, 0, 7)) %&gt;% 
  ggplot() +
  geom_density(aes(growth)) +
  labs(x = &quot;Growth rate in cm/year&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M4%20part%202-1.png" width="672" /></p>
<p>Now we just need to add a prior for sigma, the standard deviation of the height. I choose a weak prior, which is within the reasonable height space (the first plot). A sigma above 40 would lead to height values outside of this range (second plot):<br />
<span class="math inline">\(σ ∼ Uniform(0, 30)\)</span></p>
<pre class="r"><code>tibble(height = rnorm(10000, 150, 30)) %&gt;% 
  ggplot() +
  geom_density(aes(height)) +
  labs(x = &quot;Height in cm&quot;, title = &quot;sigma = 30&quot;) +
  theme_minimal()

tibble(height = rnorm(10000, 150, 40)) %&gt;% 
  ggplot() +
  geom_density(aes(height)) +
  labs(x = &quot;Height in cm&quot;, title = &quot;sigma = 40&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M4%20part%203-1.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M4%20part%203-2.png" width="50%" /></p>
</div>
<div id="question-4m5" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Question 4M5</h2>
<p><strong>Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?</strong></p>
<p>An increasing height tells us that the students are children or juveniles. We can keep our likelihood and linear model, but need to adjust the prior for alpha, beta, and sigma slightly. For alpha, we can use a new mean of 120. For beta (the growth rate), we can use a log normal distribution to force positive values and make them slightly bigger as before, as children tend to grow faster. For sigma, we can reduce it slightly as the spread is probably lower within children.</p>
<p><span class="math inline">\(h_{1} ∼ Normal(μ,σ)\)</span></p>
<p><span class="math inline">\(μ_{1} ∼ α + βx_{1}\)</span></p>
<p><span class="math inline">\(α ∼ Normal(120, 20)\)</span></p>
<p><span class="math inline">\(β = LogNormal(2, 0.5)\)</span></p>
<p><span class="math inline">\(σ ∼ Uniform(0, 20)\)</span></p>
</div>
<div id="question-4m6" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Question 4M6</h2>
<p><strong>Now suppose I tell you that the variance among heights for students of the same age is never more than 64 cm. How does this lead you to revise your priors?</strong></p>
<p>The variance is the square of σ. If the variance is never more than 64 cm, then sigma is never higher than 8 cm. We can update our prior:</p>
<p><span class="math inline">\(σ ∼ Uniform(0, 8)\)</span></p>
<p>Such a low standard deviation of height has implication for our intercept prior alpha. We are now more certain that the intercept is around 120cm and can narrow the spread a bit:</p>
<p><span class="math inline">\(α ∼ Normal(120, 10)\)</span></p>
</div>
<div id="question-4m7" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Question 4M7</h2>
<p><strong>Refit model <code>m4.3</code> from the chapter but omit the mean weight <code>xbar</code>. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is difference?</strong></p>
<p>First, let us take a look at the old model <code>m4.3</code>.</p>
<pre class="r"><code># only adults
adults &lt;- Howell1 %&gt;% filter(age &gt;= 18)

# mean weight
xbar &lt;- adults %&gt;% 
  summarise(my_mean = mean(weight)) %&gt;% 
  pull()

# fit model
m4.3 &lt;- alist(height ~ dnorm(mu, sigma), # likelihood
                 mu &lt;- a + b * (weight - xbar), # linear model
                 a ~ dnorm(178, 20), # alpha
                 b ~ dlnorm(0, 1), # beta
                 sigma ~ dunif(0, 50)) %&gt;% # sigma
  quap(data = adults) # quadratic approximation</code></pre>
<p>Now do the same but without <code>xbar</code>.</p>
<pre class="r"><code>m4.3new &lt;- alist(height ~ dnorm(mu, sigma), # likelihood
                 mu &lt;- a + b*weight, # linear model
                 a ~ dnorm(178, 20), # alpha
                 b ~ dlnorm(0, 1), # beta
                 sigma ~ dunif(0, 50)) %&gt;% # sigma
  quap(data = adults, start = list(a = 115, b = 0.9)) # quadratic approximation</code></pre>
<p>We shall look at the covariance of each model.</p>
<pre class="r"><code>m4.3new %&gt;% vcov() %&gt;% round(digits = 3) # lots of covariation</code></pre>
<pre><code>##            a      b sigma
## a      3.601 -0.078 0.009
## b     -0.078  0.002 0.000
## sigma  0.009  0.000 0.037</code></pre>
<pre class="r"><code>m4.3 %&gt;% vcov() %&gt;% round(digits = 3) # low covariation </code></pre>
<pre><code>##           a     b sigma
## a     0.073 0.000 0.000
## b     0.000 0.002 0.000
## sigma 0.000 0.000 0.037</code></pre>
<p>So we seem to increase the covariation by not centering. Let’s dig deeper by looking at summaries of the posterior distribution for each parameter:</p>
<pre class="r"><code>m4.3new %&gt;% 
  extract.samples() %&gt;% 
  as_tibble() %&gt;% 
  summarise(alpha = mean(a), beta = mean(b), sigma = mean(sigma))</code></pre>
<pre><code>## # A tibble: 1 x 3
##   alpha  beta sigma
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  115. 0.891  5.07</code></pre>
<pre class="r"><code>m4.3 %&gt;% 
  extract.samples() %&gt;% 
  as_tibble() %&gt;% 
  summarise(alpha = mean(a), beta = mean(b), sigma = mean(sigma))</code></pre>
<pre><code>## # A tibble: 1 x 3
##   alpha  beta sigma
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  155. 0.904  5.07</code></pre>
<p>While beta and sigma are the same, alpha is drastically lower in the new model without centering. But what does this mean? Before alpha was the average height for when <span class="math inline">\(x − x\)</span> was 0 (when the weight is equal to the average weight). In the new model, alpha is the average height for observation with weight equal 0. As there are no people with a weight of zero, this alpha is harder to interpret.<br />
But does this actually change the way our model works? We can test this by comparing the posterior predictions by appling our prediction functions to the data:</p>
<pre class="r"><code>m4.3.plot
m4.3new.plot</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M7%20part%206-1.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M7%20part%206-2.png" width="50%" /></p>
<p>Short answer: No, it does not. We get the same prediction intervals.</p>
</div>
<div id="question-4m8" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Question 4M8</h2>
<p><strong>In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights - change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?</strong></p>
<p>First of all, set up the model environment by importing the <code>cherry_blossoms</code> data and loading the <code>splines</code> package:</p>
<pre class="r"><code>data(&quot;cherry_blossoms&quot;)

# remove na&#39;s
cherry_blossoms &lt;- cherry_blossoms %&gt;%
  as_tibble() %&gt;%  
  drop_na()

# define nr of knots
library(splines)</code></pre>
<p>Instead of typing out the code all the time whenever we change the number of knots or the prior on the weights as needed, we make a function dependant on these two parameters and just call the function each time. All we change within the function is the number of knots and the prior on weights, everything else stays as it is. I already process and plot the output of the splines regression in the function.</p>
<pre class="r"><code># make function for it, dependant on number of knots and the prior on weights
cherry_spliner &lt;- function(nr_knots, vl_sigma){

# get knot points
knot_list &lt;- cherry_blossoms$year %&gt;% 
  quantile(probs = seq(0, 1, length.out = nr_knots)) %&gt;% 
  discard(~ . %in% c(851, 1980))

# construct basis functions
B &lt;- cherry_blossoms$year %&gt;% bs(knots = knot_list, degree = 3, intercept = TRUE) 

# run bspline regression
m4.7 &lt;- alist(D ~ dnorm(mu, sigma), 
              mu &lt;- a + B %*% w, 
              a ~ dnorm(100, 10), 
              w ~ dnorm(0, my_sigma), 
              sigma ~ dexp(1)) %&gt;% 
  quap(data = list(D = cherry_blossoms$doy, B = B, my_sigma = vl_sigma), 
                      start = list(w = rep(0, ncol(B))))

# get 97% posterior interval for mean
post_int &lt;- m4.7 %&gt;% 
  link() %&gt;% as_tibble() %&gt;% 
  map_dfr(PI, prob = 0.97) %&gt;% 
  select(&quot;lower_pi&quot; = &#39;2%&#39;, &quot;upper_pi&quot; = &#39;98%&#39;) %&gt;% 
  add_column(year = cherry_blossoms$year) %&gt;% 
  # add doy
  left_join(cherry_blossoms, by = &quot;year&quot;)

# plot it and add nr_knots and vl_sigma to plot title
ggplot(post_int, aes(year, doy)) +
  geom_point(colour = &quot;steelblue4&quot;, alpha = 0.8) +
  geom_ribbon(aes(ymin = lower_pi, ymax = upper_pi), alpha = 0.9) +
  labs(y = &quot;Day in year&quot;, x = &quot;year&quot;, 
       title = paste(nr_knots, &quot; knots, prior on weights ~ N(0,&quot;, vl_sigma, &quot;)&quot;)) +
  theme_minimal()
}</code></pre>
<p>Now we can increase the number of knots. We start with the normal model with 15 knots and sigma = 10.</p>
<pre class="r"><code>cherry_spliner(nr_knots = 15, vl_sigma = 10) 

cherry_spliner(nr_knots = 20, vl_sigma = 10)

cherry_spliner(nr_knots = 30, vl_sigma = 10)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M8%20part%203-1.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M8%20part%203-2.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M8%20part%203-3.png" width="50%" /></p>
<p>The more knots we have, the <em>wigglier</em> our trend line gets, as we capture more signal.</p>
<p>Now we can play around with the prior on weights. First, we decrease it significantly to 1, and then increase it to 100, while keeping the number of knots equal.</p>
<pre class="r"><code>cherry_spliner(nr_knots = 15, vl_sigma = 1) 

cherry_spliner(nr_knots = 15, vl_sigma = 10)

cherry_spliner(nr_knots = 15, vl_sigma = 100)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204M8%20part%204-1.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M8%20part%204-2.png" width="50%" /><img src="/post/chapter4_files/figure-html/question%204M8%20part%204-3.png" width="50%" /></p>
<p>So by increasing the prior or weights, we render our trend line more <em>wigglier</em> as well. This makes sense as the weights will be closer to 0 and hence wiggle around closer to the mean line if the standard deviation is small. If the weights become larger, we allow the curve to have more peaks.</p>
</div>
</div>
<div id="hard-practices" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Hard practices</h1>
<div id="question-4h1" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Question 4H1</h2>
<p><strong>The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">Individual</th>
<th align="center">weight</th>
<th align="right">expected height</th>
<th align="right">89% Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">46.95</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">43.72</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">64.78</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">32.59</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">54.63</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>This question is similar to the question 1 in the homework. We solved it there by using a linear regression, but this time we model the relationship between height (cm) and the natural logarithm of weight (log-kg). Let’s see if that makes any difference at all.</p>
<pre class="r"><code># new data to predict from 
new_weight &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)

# fit the logarithmic model
m_log &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * log(weight),
  a ~ dnorm(178, 20),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)) %&gt;% 
  quap(data = d, 
       start = list(a = mean(d$height), b = 1.5, sigma = 9))</code></pre>
<p>Now we can make predictions from our model:</p>
<pre class="r"><code># predict height 
pred_height &lt;- link(m_log, data = data.frame(weight = new_weight))

# calculate means
expected &lt;- pred_height %&gt;% 
  as_tibble() %&gt;% 
  summarise_all(mean) %&gt;% 
  as_vector()

# calculate percentile interval
interval &lt;- pred_height %&gt;% 
  as_tibble() %&gt;% 
  summarise_all(HPDI, prob = 0.89) %&gt;% 
  as_vector()</code></pre>
<p>And add the predictions to the table.</p>
<pre class="r"><code>tibble(individual = 1:5, weight = new_weight, expected = expected, 
       lower = interval[c(TRUE, FALSE)], upper = interval[c(FALSE, TRUE)]) %&gt;% 
  knitr::kable(align = &quot;cccrr&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">individual</th>
<th align="center">weight</th>
<th align="center">expected</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">46.95</td>
<td align="center">157.3319</td>
<td align="right">156.8660</td>
<td align="right">157.7259</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">43.72</td>
<td align="center">153.9949</td>
<td align="right">153.5659</td>
<td align="right">154.3738</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">64.78</td>
<td align="center">172.4026</td>
<td align="right">171.7798</td>
<td align="right">172.9538</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">32.59</td>
<td align="center">140.2403</td>
<td align="right">139.9027</td>
<td align="right">140.6235</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">54.63</td>
<td align="center">164.4245</td>
<td align="right">163.9121</td>
<td align="right">164.8932</td>
</tr>
</tbody>
</table>
<p>And now we can compare our results to the predictions from the regular model, which we named <code>table_height</code>.</p>
<pre class="r"><code>table_height</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">individual</th>
<th align="center">weight</th>
<th align="center">expected</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">46.95</td>
<td align="center">158.2962</td>
<td align="right">157.4396</td>
<td align="right">159.1011</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">43.72</td>
<td align="center">152.5978</td>
<td align="right">151.8108</td>
<td align="right">153.3172</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">64.78</td>
<td align="center">189.7522</td>
<td align="right">188.3641</td>
<td align="right">191.2205</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">32.59</td>
<td align="center">132.9621</td>
<td align="right">132.3597</td>
<td align="right">133.6208</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">54.63</td>
<td align="center">171.8454</td>
<td align="right">170.9054</td>
<td align="right">173.0321</td>
</tr>
</tbody>
</table>
<p>And we can see that it actually makes a big difference, especially for those with a large weight (<em>individual 3</em>), or with a particularly low weight (<em>individual 4</em>).</p>
</div>
<div id="question-4h2" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Question 4H2</h2>
<p><strong>Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it.</strong></p>
<pre class="r"><code>young &lt;- d %&gt;% filter(age &lt; 18)

# double-check
str(young)</code></pre>
<pre><code>## &#39;data.frame&#39;:    192 obs. of  4 variables:
##  $ height: num  121.9 105.4 86.4 129.5 109.2 ...
##  $ weight: num  19.6 13.9 10.5 23.6 16 ...
##  $ age   : num  12 8 6.5 13 7 17 16 11 17 8 ...
##  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...</code></pre>
<div id="part-1" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Part 1</h3>
<p><strong>Fit a linear regression to these data, using <code>quap()</code>. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?</strong></p>
<p>We can use the same priors as before, but should decrease the prior on alpha to account for lower overall height in our (non-adult) data set. It is important to center the weight values for interpretation.</p>
<pre class="r"><code># again, get mean weight for centering
xbar &lt;- young %&gt;% summarise(xbar = mean(weight)) %&gt;% pull

m_young &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * (weight - xbar),
  a ~ dnorm(120, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 60)) %&gt;% 
  quap(data = young)

# results
m_young_res &lt;- precis(m_young) %&gt;% 
  as_tibble() %&gt;% 
  add_column(parameter = rownames(precis(m_young)), .before = &quot;mean&quot;) %&gt;% 
  rename(&quot;lower&quot; = &#39;5.5%&#39;, &quot;upper&quot; = &#39;94.5%&#39;)

knitr::kable(m_young_res)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="right">108.323275</td>
<td align="right">0.6089240</td>
<td align="right">107.350097</td>
<td align="right">109.296453</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="right">2.716610</td>
<td align="right">0.0683322</td>
<td align="right">2.607402</td>
<td align="right">2.825818</td>
</tr>
<tr class="odd">
<td align="left">sigma</td>
<td align="right">8.439234</td>
<td align="right">0.4308276</td>
<td align="right">7.750689</td>
<td align="right">9.127780</td>
</tr>
</tbody>
</table>
<p>b can be interpreted as the slope in our regression, where with 1 unit change, height increases by 2.72. Hence, for every 10 units of increase in weight the model predicts that a child gets 27.2 cm taller.</p>
</div>
<div id="part-2" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Part 2</h3>
<p><strong>Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the <code>quap</code> regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.</strong></p>
<p>And this is the reasion why we build our functions for this. We don’t need to do it manually all over again:</p>
<pre class="r"><code># define weight range
weight_seq &lt;- seq(from = min(young$weight), to = max(young$weight), by = 1)

# calculate 89% intervals for each weight
intervals &lt;- tidy_intervals(link, m_young, HPDI, 
                            x_var = &quot;weight&quot;, x_seq = weight_seq)

# calculate means
reg_line &lt;- tidy_mean(m_young, data = data.frame(weight = weight_seq))

# calculate prediction intervals
pred_intervals &lt;- tidy_intervals(sim, m_young, PI, 
                                 x_var = &quot;weight&quot;, x_seq = weight_seq)

plot_regression(young, weight, height) +
  labs(title = &quot;Linear model (m_young)&quot;)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H2%20part%203-1.png" width="672" /></p>
</div>
<div id="part-3" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Part 3</h3>
<p><strong>What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.</strong></p>
<p>The model clearly fails to estimate height at both low (&lt;10) and high (&gt;30) weights. By just eyeballing the data, it seems that a linear relationship assumption is not realistic. To improve the model, we can either transform one of the parameters, or use a polynomial regression.</p>
</div>
</div>
<div id="question-4h3" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Question 4H3</h2>
<p><strong>Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.</strong></p>
<div id="part-1-1" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Part 1</h3>
<p><strong>Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Can you interpret the resulting estimates.?</strong></p>
<p>All we need to do is make a formula for the log of the weights, and the fit it to <code>quap()</code>. Remember that this uses all the data, so we should adjust our priors to it. Then we get the coefficient with <code>precis()</code>.</p>
<pre class="r"><code># fit model
m_log &lt;- alist(
  height ~ dnorm(mu, sigma),
  mu &lt;- a + b * log(weight),
  a ~ dnorm(178, 100),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)) %&gt;% 
  quap(data = d)

# get coefficients
m_log_res &lt;- precis(m_log) %&gt;% as_tibble() %&gt;% 
  add_column(parameter = rownames(precis(m_log)), .before = &quot;mean&quot;) %&gt;% 
  rename(&quot;lower&quot; = &#39;5.5%&#39;, &quot;upper&quot; = &#39;94.5%&#39;)

knitr::kable(m_log_res, align = &quot;lccc&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="center">mean</th>
<th align="center">sd</th>
<th align="center">lower</th>
<th align="left">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="center">-23.734756</td>
<td align="center">1.3353063</td>
<td align="center">-25.868833</td>
<td align="left">-21.600679</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="center">47.060955</td>
<td align="center">0.3826012</td>
<td align="center">46.449485</td>
<td align="left">47.672426</td>
</tr>
<tr class="odd">
<td align="left">sigma</td>
<td align="center">5.134726</td>
<td align="center">0.1556704</td>
<td align="center">4.885935</td>
<td align="left">5.383517</td>
</tr>
</tbody>
</table>
<p>We get a weird alpha estimate of -24, what does this mean? It’s just the predicted height of an individual with the weight of 0 log-kg. Beta shows the predicted increase (41 cm) for a 1 log-kg increase in weight. The standard deviation of height prediction, sigma, is around 5 cm. We can see that using a transformation for one parameter renders the coefficients less interpretable.</p>
</div>
<div id="part-2-1" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Part 2</h3>
<p><strong>Begin with this plot: <code>plot(height ~ weight, data = Howell1), col = col.alpha(rangi2, 0.4))</code>. Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.</strong></p>
<p>Again, our predefined functions come in quite handy here. But first let’s recreate the plot in <code>ggplot2</code>.</p>
<pre class="r"><code>ggplot(data = d) +
  geom_point(aes(x = weight, y = height), colour = &quot;steelblue4&quot;, alpha = 0.5) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H3%20part%202-1.png" width="672" /></p>
<p>Now we can calculate the predictions from our model:</p>
<pre class="r"><code># define weight range
weight_seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 1)

# calculate 89% intervals for each weight
intervals &lt;- tidy_intervals(link, m_log, HPDI, 
                            x_var = &quot;weight&quot;, x_seq = weight_seq)

# calculate means
reg_line &lt;- tidy_mean(m_log, data = data.frame(weight = weight_seq))

# calculate prediction intervals
pred_intervals &lt;- tidy_intervals(sim, m_log, PI, 
                                 x_var = &quot;weight&quot;, x_seq = weight_seq)

# plot it 
plot_regression(d, weight, height)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H3%20part%203-1.png" width="672" /></p>
<p>We can see a pretty good fit for the relationship between height (cm) and the natural logarithm of weight (log-kg).</p>
</div>
</div>
<div id="question-4h4" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Question 4H4</h2>
<p><strong>Plot the prior predictive distribution for the parabolic polynomial regression model by modifying the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of a, b1, and b2 so that the prior predictions stay withing the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consisten with what you know about height and weight, before seeing these exact data.</strong></p>
<p>We already covered this in <em>homework question 3</em> above.</p>
</div>
<div id="question-4h5" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Question 4H5</h2>
<p><strong>Return to data(cherry_blossoms) and model the association between blossom date (<code>doy</code>) and March temperature (<code>temp</code>). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature rend predict the blossom trend?</strong></p>
<p>The easiest way to deal with missing values is to omit them. There are more sophisticated ways but for now it’s sufficient for us. Let’s load the data, select the parameters and add a column with the centered temperature in case we apply a polynomial regression.</p>
<pre class="r"><code>data(&quot;cherry_blossoms&quot;)

cherry_blossoms &lt;- cherry_blossoms %&gt;%
  as_tibble() %&gt;% 
  select(doy, temp) %&gt;% 
  drop_na() %&gt;% 
  mutate(temp_sc = (temp - mean(temp))/ sd(temp))</code></pre>
<p>Now let’s take a quick look at the data to choose which model to use.</p>
<pre class="r"><code>ggplot(cherry_blossoms) +
  geom_point(aes(temp_sc, doy), alpha = 0.5) +
  labs(x = &quot;Centered temperature&quot;, y = &quot;Day in year&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H5%20part%203-1.png" width="672" /></p>
<p>It seems like there is a nuanced negative relationship, but with a lot of noise. Using a polynomial regression or a spline would probably capture too much of this noise without providing much insight. I will therefore use a simple linear regression approach and the temperature parameter as it is (without centering). I will mainly use less informative, wide priors. There is, however enough data so that priors do not matter that much. The prior on alpha covers most realistic values and the prior on beta that favours negative values (= negative slope = negative relationship).</p>
<pre class="r"><code># define average temp
xbar &lt;- cherry_blossoms %&gt;% 
  summarise(mean_temp = mean(temp)) %&gt;% 
  pull()

# fit modell
cherry_linear &lt;- 
  # define formula
  alist(doy ~ dnorm(mu, sigma),
        mu &lt;- a + b * (temp - xbar),
        a ~ dnorm(115, 30),
        b ~ dnorm(-2, 5),
        sigma ~ dunif(0, 50)) %&gt;% 
  # calculate maximum a posterior
  quap(data = cherry_blossoms)</code></pre>
<p>Let’s glimpse at the estimated parameters:</p>
<pre class="r"><code>precis(cherry_linear) %&gt;% as_tibble() %&gt;% 
  add_column(parameter = rownames(precis(cherry_linear))) %&gt;% 
  rename(&quot;lower&quot; = &#39;5.5%&#39;, &quot;upper&quot; = &#39;94.5%&#39;) %&gt;% 
  select(parameter, everything()) %&gt;% 
  knitr::kable(align = &quot;lcrrr&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="center">mean</th>
<th align="right">sd</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="center">104.921601</td>
<td align="right">0.2106748</td>
<td align="right">104.584902</td>
<td align="right">105.258300</td>
</tr>
<tr class="even">
<td align="left">b</td>
<td align="center">-2.990052</td>
<td align="right">0.3078881</td>
<td align="right">-3.482117</td>
<td align="right">-2.497987</td>
</tr>
<tr class="odd">
<td align="left">sigma</td>
<td align="center">5.910315</td>
<td align="right">0.1489851</td>
<td align="right">5.672208</td>
<td align="right">6.148422</td>
</tr>
</tbody>
</table>
<p>The intercept is around day 105. The relationship is indeed negative: With every degree celsius warmer, the day of blossom is 3 days earlier on average. The confidence intervals are negative as well, showing that this relationship is somewhat strong (puhh, I almost said <em>significant</em>).<br />
Let’s look at the 89% prediction intervals:</p>
<pre class="r"><code># define sequence of temperature
temp_seq &lt;- seq(from = min(cherry_blossoms$temp), 
                to = max(cherry_blossoms$temp), by = 0.5)

# calculate 89% intervals for each weight
intervals &lt;- tidy_intervals(link, cherry_linear, HPDI, &quot;temp&quot;, temp_seq)


# calculate means
reg_line &lt;- tidy_mean(cherry_linear, data = data.frame(temp = temp_seq))


# calculate prediction intervals
pred_intervals &lt;- tidy_intervals(sim, cherry_linear, PI, &quot;temp&quot;, temp_seq)

  
# plot it 
plot_regression(cherry_blossoms, temp, doy)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H5%20part%206-1.png" width="672" /></p>
<p>We can see the (coloured) negative trend line. The darker shaded interval around it shows the 89% plausible regions for the distribution of the mean <code>doy</code>. The lighter and broader interval shows the region within which the model expects to find 89% of actual height in the population. So to answer the question: Temperature is clearly associated with the blossom trend, but the scatter of the predictions is quite large. Temperature does a mediocre job in predicting the <code>doy</code>.</p>
</div>
<div id="question-4h6" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Question 4H6</h2>
<p><strong>Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weight is doing?</strong></p>
<p>I had quite a hard time to wrap my head around this question. I think that we are supposed to simulate the prior predictive distribution for <code>doy</code> (Day in year), wich is the outcome variable of the linear model. This was quite easy in previous examples, as we just had to sample from each prior distribution and combine the results. In this case, however, we have to implement the basis functions.<br />
But let’s first load the data again.</p>
<pre class="r"><code>data(&quot;cherry_blossoms&quot;)

cherry_blossoms &lt;- cherry_blossoms %&gt;%
  as_tibble() %&gt;% 
  select(doy, year) %&gt;% 
  drop_na(doy)</code></pre>
<p>Now let’s recreate the knots and the basis functions for the spline:</p>
<pre class="r"><code># number of knots
nr_knots &lt;- 15

knot_list &lt;- cherry_blossoms$year %&gt;% 
  quantile(probs = seq(0, 1, length.out = nr_knots)) 

# construct basis functions
B &lt;- cherry_blossoms$year %&gt;% 
  bs(knots = knot_list, degree = 3, intercept = TRUE) </code></pre>
<p>And here the hard part begins. The linear model is given on page 117:<br />
<code>mu &lt;- a + B %*% w</code>.</p>
<p>We have to multiply each element of w by each value in the corresponding row of B and then sum each result, and add it to a.</p>
<p>Now let’s first calculate the prior predictive distribution with the prior used in the chapter. For this, we sample from the priors and calculate the trend line using the linear model equation. Note that we use the basis functions calculated above (B).<br />
Let’s first do it once by sampling a and w and then calculating mu:</p>
<pre class="r"><code>N &lt;- ncol(B)

a &lt;- rnorm(N, 100, 10)
w &lt;- rnorm(N, 0, 10)
    
mu &lt;- purrr::map(1:nrow(B), ~ a + sum(B[.x, ]*w))</code></pre>
<p>mu is a list with 827 elements (one for each year observation in the data), and each element contains 19 estimates for mu (one for each knot/ basis function). We can get the mean estimate for each knot by applying the <code>mean</code> function to each element in mu:</p>
<pre class="r"><code>mu &lt;- map_dbl(mu, mean)</code></pre>
<p>Now mu is a vector with length 827, giving us the mean estimate for one spline trend line:</p>
<pre class="r"><code>cherry_blossoms %&gt;% 
  ggplot() + 
  geom_point(aes(year, doy), colour = &quot;steelblue2&quot;, alpha = 0.7) +
  geom_path(aes(year, value), 
            data = mu %&gt;% as_tibble() %&gt;% 
              add_column(year = cherry_blossoms$year), 
            colour = &quot;coral&quot;, size = 1.3) +
  labs(x = &quot;Day in year&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%205-1.png" width="672" /></p>
<p>This is just one guess of the model for mean <code>doy</code> without seeing any data. But for a proper prior predictive simulation, we want to see many guesses of the model to get the overall distribution. For this, we need to write our sampling into a function with i as an argument. i will be later used to replicate the function.</p>
<pre class="r"><code>mu_sampler &lt;- function(i){
    a &lt;- rnorm(N, 100, 10)
    w &lt;- rnorm(N, 0, 10)
    
    mu &lt;- purrr::map(1:nrow(B), ~ a + sum(B[.x, ]*w)) %&gt;% 
      map_dbl(mean)
  }</code></pre>
<p>Now we can simulate 1,000 runs using <code>replicate</code>.</p>
<pre class="r"><code>mu2 &lt;- replicate(1e3, mu_sampler()) %&gt;% 
  as.vector() %&gt;% 
  as_tibble() 
  
  
ggplot(mu2) +
  geom_density(aes(value)) +
  theme_minimal() +
  labs(title = paste0(&quot;w ~ dnorm(&quot;, 0, &quot;,&quot;, 10, &quot;)&quot;), 
         x = &quot;Day in year&quot;)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%207-1.png" width="672" /></p>
<p>This is the prior predictive distribution for <code>doy</code> with the prior on w used in the chapter. The model predicts <code>doy</code> to be normally distributed around the day 100.
But we are supposed to play around with the prior on w and see the effect on the prior predictive distribution. So let’s make a function from the above code dependent on the prior values:</p>
<pre class="r"><code>prior_weight &lt;- function(w_mean, w_sd) {
  
  mu_sampler &lt;- function(i){
    a &lt;- rnorm(N, 100, 10)
    w &lt;- rnorm(N, w_mean, w_sd)
    
    mu &lt;- purrr::map(1:nrow(B), ~ a + sum(B[.x, ]*w)) %&gt;% 
      map_dbl(mean)
  }
  
  mu2 &lt;- replicate(1e3, mu_sampler()) %&gt;% 
    as.vector() %&gt;% 
    as_tibble() 
  
  
  ggplot(mu2) +
    geom_density(aes(value)) +
    theme_minimal() +
    labs(title = paste0(&quot;w ~ dnorm(&quot;, w_mean, &quot;,&quot;, w_sd, &quot;)&quot;), 
         x = &quot;Day in year&quot;)
}</code></pre>
<p>Now what happens if we increase the standard deviaton of w?</p>
<pre class="r"><code>prior_weight(0, 20)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%209-1.png" width="672" /></p>
<p>We get more uncertainty for <code>doy</code>. Let’s see what happens if we increase the mean instead:</p>
<pre class="r"><code>prior_weight(10, 10)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%2010-1.png" width="672" /></p>
<p>We shift the distribution to the right. So to be honest I am not sure what w is doing precisely, as it is quite hard to read that from the prior predictive distribution where all knots are combined. Instead, let’s try a prior predictive simulation for each knot.<br />
First, set up a function that calculates the mean <code>doy</code>for each knot based on the priors used in the chapter:</p>
<pre class="r"><code>knot_sampler &lt;- function(i){
  a &lt;- rnorm(N, 100, 10)
  w &lt;- rnorm(N, 0, 10)
  
  mu &lt;- purrr::map(1:nrow(B), ~ a + sum(B[.x, ]*w)) %&gt;% 
    map_dbl(mean)
}</code></pre>
<p>Now repeat the sampling for <code>doy</code> for each knot a thousand times:</p>
<pre class="r"><code>mu2 &lt;- replicate(1e3, knot_sampler()) %&gt;% 
  as_tibble() %&gt;% 
  magrittr::set_colnames(1:1e3)</code></pre>
<p>Let’s calculate the mean and the PI for each knot from these thousand samples:</p>
<pre class="r"><code>mu2_rep &lt;- mu2 %&gt;% 
  add_column(year = cherry_blossoms$year) %&gt;% 
  pivot_longer(cols = -year, 
               names_to = &quot;trial&quot;, values_to = &quot;doy&quot;) %&gt;% 
  group_by(year) %&gt;% 
  summarise(mean_doy = mean(doy), 
            lower_val = PI(doy)[1], 
            upper_val = PI(doy)[2]) </code></pre>
<p>And plot it:</p>
<pre class="r"><code>ggplot(data = mu2_rep) + 
  geom_point(aes(year, doy), 
             colour = &quot;steelblue2&quot;, alpha = 0.7, 
             data = cherry_blossoms,) +
  geom_ribbon(aes(x = year, ymin = lower_val, ymax = upper_val),
                alpha = 0.3) +
  geom_path(aes(year, mean_doy), 
            colour = &quot;coral&quot;, size = 1.3) +
  labs(title = paste0(&quot;w ~ dnorm(&quot;, 0, &quot;,&quot;, 10, &quot;)&quot;), 
       y = &quot;Day in year&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%2014-1.png" width="672" /></p>
<p>We can see that we assign the same prior distribution for each knot (flat line of the mean and the percentile interval borders), but with some edge effects. Now let’s tinker with w. First, we again make a function dependent on w:</p>
<pre class="r"><code>prior_knot &lt;- function(w_mean, w_sd) {
  knot_sampler &lt;- function(i){
  a &lt;- rnorm(N, 100, 10)
  w &lt;- rnorm(N, w_mean, w_sd)
  
  mu &lt;- purrr::map(1:nrow(B), ~ a + sum(B[.x, ]*w)) %&gt;% 
    map_dbl(mean)
  }
  
  mu2 &lt;- replicate(1e3, knot_sampler()) %&gt;%
    as_tibble() %&gt;% 
    magrittr::set_colnames(1:1e3)
  
  mu2_rep &lt;- mu2 %&gt;% 
    add_column(year = cherry_blossoms$year) %&gt;% 
    pivot_longer(cols = -year, 
               names_to = &quot;trial&quot;, values_to = &quot;doy&quot;) %&gt;% 
    group_by(year) %&gt;% 
    summarise(mean_doy = mean(doy), 
            lower_val = PI(doy)[1], 
            upper_val = PI(doy)[2])
  
  ggplot(data = mu2_rep) + 
    geom_point(aes(year, doy), 
             colour = &quot;steelblue2&quot;, alpha = 0.7, 
             data = cherry_blossoms,) +
    geom_ribbon(aes(x = year, ymin = lower_val, ymax = upper_val),
                alpha = 0.3) +
    geom_path(aes(year, mean_doy), 
            colour = &quot;coral&quot;, size = 1.3) +
    labs(title = paste0(&quot;w ~ dnorm(&quot;, w_mean, &quot;,&quot;, w_sd, &quot;)&quot;), 
       y = &quot;Day in year&quot;) +
    theme_minimal()
}</code></pre>
<p>And now let’s increase the standard deviation:</p>
<pre class="r"><code>prior_knot(0, 20)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%2016-1.png" width="672" /></p>
<p>Increasing the sd for w increases the spread of the percentile interval, while the mean stays the same. What happens when we increase the mean of w?</p>
<pre class="r"><code>prior_knot(10, 10)</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H6%20part%2017-1.png" width="672" /></p>
<p>Now we increase the overall estimate for <code>doy</code>. It seems that w acts on each knot equally. It further seems to drive the overall estimate for <code>doy</code>.</p>
</div>
<div id="question-4h7" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Question 4H7</h2>
<p><strong>The cherry blossom spline in the chapter used an intercept a, but technically it doesn’t require one. The first basis function could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work?</strong></p>
<p>Let’s just try it without an intercept:</p>
<pre class="r"><code>no_int &lt;- alist(
    D ~ dnorm(mu, sigma), 
    mu &lt;- B %*% w, 
    w ~ dnorm(0, 10), 
    sigma ~ dexp(1)) %&gt;% 
  quap(
   data = list(D = cherry_blossoms$doy, B = B), 
   start = list(w = rep(0, ncol(B))))</code></pre>
<p>Let’s see what the model says by sampling from the posterior distribution.</p>
<pre class="r"><code>mu &lt;- link(no_int)

# calculate mean
mu_mean &lt;- mu %&gt;% 
  as_tibble() %&gt;%
  purrr::map_dbl(mean) %&gt;% 
  enframe(name = &quot;year&quot;, value = &quot;mean_doy&quot;) %&gt;% 
  mutate(year = cherry_blossoms$year)

# calculate 89% posterior interval
mu_pi &lt;- mu %&gt;% 
  as_tibble() %&gt;%
  purrr::map(PI) %&gt;% 
  enframe(name = &quot;year&quot;, value = &quot;PI&quot;) %&gt;% 
  mutate(lower_val = map_dbl(PI, 1), 
         upper_val = map_dbl(PI, 2), 
         year = cherry_blossoms$year) %&gt;% 
  select(-PI) </code></pre>
<p>We can combine the mean and the pi and plot it:</p>
<pre class="r"><code>mu_mean %&gt;% 
  left_join(mu_pi) %&gt;% 
  ggplot() +
  geom_point(aes(year, doy), 
             colour = &quot;steelblue2&quot;, alpha = 0.3, 
             data = cherry_blossoms) +
  geom_ribbon(aes(x = year, ymin = lower_val, ymax = upper_val), 
              alpha = 0.3) +
  geom_path(aes(year, mean_doy), 
            colour = &quot;coral&quot;, alpha = 0.95, size = 1) +
  labs(title = &quot;Model with no intercept&quot;, y = &quot;Day in year&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/chapter4_files/figure-html/question%204H7%20part%203-1.png" width="672" /></p>
<p>To be honest, that looks pretty convincing to me. And it obviously worked fine, so I will not change anything.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] splines   parallel  stats     graphics  grDevices utils     datasets 
## [8] methods   base     
## 
## other attached packages:
##  [1] rethinking_2.01      dagitty_0.3-0        rstan_2.21.2        
##  [4] StanHeaders_2.21.0-6 forcats_0.5.0        stringr_1.4.0       
##  [7] dplyr_1.0.2          purrr_0.3.4          readr_1.4.0         
## [10] tidyr_1.1.2          tibble_3.0.4         ggplot2_3.3.2       
## [13] tidyverse_1.3.0     
## 
## loaded via a namespace (and not attached):
##  [1] matrixStats_0.57.0 fs_1.5.0           lubridate_1.7.9    httr_1.4.2        
##  [5] tools_4.0.2        backports_1.1.10   utf8_1.1.4         R6_2.4.1          
##  [9] DBI_1.1.0          colorspace_1.4-1   withr_2.3.0        tidyselect_1.1.0  
## [13] gridExtra_2.3      prettyunits_1.1.1  processx_3.4.4     curl_4.3          
## [17] compiler_4.0.2     cli_2.1.0          rvest_0.3.6        xml2_1.3.2        
## [21] labeling_0.3       bookdown_0.20      scales_1.1.1       mvtnorm_1.1-1     
## [25] callr_3.5.0        digest_0.6.26      rmarkdown_2.4      pkgconfig_2.0.3   
## [29] htmltools_0.5.0    dbplyr_1.4.4       highr_0.8          rlang_0.4.8       
## [33] readxl_1.3.1       rstudioapi_0.11    shape_1.4.5        generics_0.0.2    
## [37] farver_2.0.3       jsonlite_1.7.1     inline_0.3.16      magrittr_1.5      
## [41] loo_2.3.1          Rcpp_1.0.5         munsell_0.5.0      fansi_0.4.1       
## [45] lifecycle_0.2.0    stringi_1.5.3      yaml_2.2.1         MASS_7.3-51.6     
## [49] pkgbuild_1.1.0     grid_4.0.2         blob_1.2.1         crayon_1.3.4      
## [53] lattice_0.20-41    haven_2.3.1        hms_0.5.3          knitr_1.30        
## [57] ps_1.4.0           pillar_1.4.6       boot_1.3-25        codetools_0.2-16  
## [61] stats4_4.0.2       reprex_0.3.0       glue_1.4.2         evaluate_0.14     
## [65] blogdown_0.21      V8_3.2.0           RcppParallel_5.0.2 modelr_0.1.8      
## [69] vctrs_0.3.4        cellranger_1.1.0   gtable_0.3.0       assertthat_0.2.1  
## [73] xfun_0.18          broom_0.7.1        coda_0.19-4        ellipsis_0.3.1</code></pre>
</div>
</div>
