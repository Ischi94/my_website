---
title: "Wilcoxon signed-rank test"
author: "Gregor Mathes"
date: "2021-04-01"
slug: Regression tests
categories: []
tags: [Regression in disguise, Regression, t-test, Bayes, Statistics]
subtitle: ''
summary: 'Modeling the central tendency within one sample without the normality assumption'
authors: [Gregor Mathes]
lastmod: "01 April, 2021"
featured: no
projects: [Regression tests]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: false
    fig_width: 6
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#wilcoxon-signed-rank-test">Wilcoxon signed-rank test</a></li>
<li><a href="#comparing-the-test-to-the-regression">Comparing the test to the regression</a></li>
<li><a href="#bayesian-estimation">Bayesian estimation</a></li>
<li><a href="#comparison">Comparison</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This is the second post for my series on statistical tests as regression models. I want to show that each classical stats test such as an ANOVA or a t-test are just linear regression models in disguise. After showing this concept for each test, I aim to build a Bayesian regression for each that supersedes both the classical test and the frequentist regression approach. You can find the other parts from the <em>statistical tests as regression</em> series <a href="https://gregor-mathes.netlify.app/tags/regression-in-disguise/">here</a>.<br />
In this post, I will cover the non-parametric sibling of the one sample t-test, the Wilcoxon signed-rank test. I will use this opportunity to introduce rank-transformations for non-normal data.</p>
</div>
<div id="wilcoxon-signed-rank-test" class="section level1">
<h1>Wilcoxon signed-rank test</h1>
<div id="when-to-use" class="section level2">
<h2>When to use</h2>
<p>Similar to the one sample t-test (which I have covered in <a href="https://gregor-mathes.netlify.app/2021/03/27/regression-tests/">another blog post</a>), the Wilcoxon signed-rank test can be applied to estimate the central tendency of a sample (e.g. the mean) compared to a specific value. For example, if I want to estimate whether the average IQ of my colleagues at University Bayreuth is above the overall average IQ of 100. The good thing about the Wilcoxon signed-rank test compared to the t-test is the relaxation of the assumption of normality. If you have evidence that the distribution of the response variable in your sample is non-normally distributed, you typically apply the Wilcoxon signed-rank test.</p>
</div>
<div id="how-does-it-work" class="section level2">
<h2>How does it work?</h2>
<p>The one sample t-test belongs to the parametric tests. That means that the test makes assumption about your data, and only under those assumptions the test will return reliable results. The Wilcoxon signed-rank test on the other hand is non-parametric and does not care about the underlying distribution. But how does that work? It turns out that within the black box of the test, all our data get transformed. Before the test is evaluated, the data gets ordered in signed ranks (hence the name). After transformation, the data is more or less Gaussian shaped, and then thrown in a one sample t-test. So the Wilcoxon signed-rank test is just a one sample t-test with some data transformation beforehand. And as we know how the t-test looks as a regression, we can quite easily disentangle the black box of the Wilcoxon test. But first, let’s look at how signed rank transformation works.</p>
</div>
<div id="signed-rank-transformation" class="section level2">
<h2>Signed rank transformation</h2>
<p>If you have some data points, you can order them via their ranks from low to high. In R, you can do this via the <code>rank()</code> function.</p>
<pre class="r"><code>dat &lt;- c(1, 5, 3, -2, 20, -5)

dat %&gt;% 
  rank()</code></pre>
<pre><code>## [1] 3 5 4 2 6 1</code></pre>
<p>The lowest number (-5 in the example) gets assigned the rank 1. Now the signed rank transformation works quite similar. We just take the absolute of the data, rank it and then assign each data point the same sign it has before we took the absolute.</p>
<pre class="r"><code>dat %&gt;% 
  abs() %&gt;% 
  rank() * sign(dat)</code></pre>
<pre><code>## [1]  1.0  4.5  3.0 -2.0  6.0 -4.5</code></pre>
<p>But why bother? It turns out that this transformation is able to make a non-normal distribution normally shaped. And this then allows us to apply various parametric tests (you shouldn’t though, but let’s keep that for later).
We can show this by sampling from a number of non-normal data, and then try to make them Gaussian shaped by applying the signed-rank transformation.</p>
<pre class="r"><code># simulate data from various distributions
simulated_data &lt;- tibble(log_normal = rlnorm(1e5), 
       poisson = rpois(1e5, 10), 
       binomial = rbinom(1e5, 10, 0.5), 
       exponential = rexp(1e5)) %&gt;% 
  pivot_longer(cols = everything(), 
               names_to = &quot;distribution&quot;, 
               values_to = &quot;value&quot;) 

# plot untransformed data
simulated_data %&gt;% 
  ggplot(aes(value)) +
  geom_density(fill = &quot;grey50&quot;, colour = &quot;grey10&quot;, 
               alpha = 0.9) +
  facet_wrap(~distribution, scale = &quot;free&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/simulation%20raw-1.png" width="576" /></p>
<p>So that’s the un-transformed simulated data. Now let’s try to transform them.</p>
<pre class="r"><code># define the transformation function
signed_rank &lt;-  function(x) {
  sign(x) * rank(abs(x))
}

# apply the transformation to each distribution 
simulated_data %&gt;% 
  group_by(distribution) %&gt;%
  mutate(trans_value = signed_rank(value)) %&gt;% 
  ggplot(aes(trans_value)) +
  geom_density(fill = &quot;grey50&quot;, colour = &quot;grey10&quot;, 
               alpha = 0.9) +
  facet_wrap(~distribution, scale = &quot;free&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/simulation%20transformed-1.png" width="576" /></p>
<p>We can see that the transformation does not work properly on all distributions (especially for the binomial and the poisson data). This is because the rank-transformation works only for continuous data, and not for discrete. And on the other distributions, it seems like a mixture of a normal and a uniform distribution. The test is applied as a black box and you don’t see these transformations, and that those transformations might fail. We will see how to overcome these problems with Bayesian estimation later on in this blog. But not only Bayesian estimation is capable to tackle these issues. There are frequentist ways for regression on discrete data as well. But all of this requires thinking, cross-checking, and plotting each step in the analysis. You won’t tackle these issues with a classical test like. Even worse, if you throw the data in such a black box, it won’t even tell you that something is wrong, or that it’s assumptions are violated.</p>
</div>
<div id="assumptions" class="section level2">
<h2>Assumptions</h2>
<p>Even though the Wilcoxon signed-rank test relaxes the normality assumption of the one sample t-test, it still relies on a number of assumption.</p>
<ol style="list-style-type: decimal">
<li>continuous (not discrete) and measured on an interval scale<br />
</li>
<li>normally distributed<br />
</li>
<li>a random sample from its population with each individual in the population having an equal
probability of being selected in the sample</li>
</ol>
</div>
<div id="as-a-regression" class="section level2">
<h2>As a regression</h2>
<p>We can simply take the regression formula for the one sample t-test after applying the signed-rank transformation. So all we need is a regression with an intercept and nothing less. We can state this as <span class="math display">\[\mu = \alpha\]</span> where <span class="math inline">\(\alpha\)</span> is the intercept and <span class="math inline">\(\mu\)</span> the height of each individual.</p>
</div>
</div>
<div id="comparing-the-test-to-the-regression" class="section level1">
<h1>Comparing the test to the regression</h1>
<div id="the-data" class="section level2">
<h2>The data</h2>
<p>I will generate data for chess games, because according to a stackexchange post, <a href="https://chess.stackexchange.com/questions/2506/what-is-the-average-length-of-a-game-of-chess/4899#4899">The length of chess games tends to follow a log-normal distribution</a>. If you would plot the data first and realize that it comes from a log-normal, you could easily transform it to a Gaussian curve instead of using rank-transformation. This would then result in an improved estimation compared to the Wilcoxon test. But we will come to that in a second. Here’s the data, with values taken from the stackexchange post:</p>
<pre class="r"><code>set.seed(1708)
dat_chess &lt;- rlnorm(1e5, 4.37, 0.45)

dat_chess %&gt;% 
  as_tibble() %&gt;% 
  ggplot(aes(value)) +
  geom_density(fill = &quot;grey50&quot;, colour = &quot;grey10&quot;, 
               alpha = 0.9) +
  labs(x = &quot;Length of chess game&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/sim%20data-1.png" width="576" /></p>
<p>With this much data, all our p-value would be totally small (see the problem?). So I will sample 200 values from this total distribution.</p>
<pre class="r"><code>set.seed(1708)
dat_chess &lt;- sample(dat_chess, 200)</code></pre>
<p>Our hypothesis is that the average chess game from this sample takes more than 75 minutes. Why? Because my total play time for online chess averages around this value, and I want to see where I am in terms of chess game longevity.</p>
</div>
<div id="the-black-box" class="section level2">
<h2>The black box</h2>
<p>We can just throw the data in the Wilcoxon signed-rank test and define the value we want to compare with <code>mu</code>. We can set our hypothesis specified above via <code>alternative = "greater"</code>.</p>
<pre class="r"><code>wilcox.test(dat_chess, mu = 75, alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  dat_chess
## V = 12200, p-value = 0.004361
## alternative hypothesis: true location is greater than 75</code></pre>
<p>We get a p-value below a arbitrarily chosen threshold and are hence able to reject the null hypothesis that our sample mean is equal or below 75 minutes. However, we should not stick to p-values and instead report effect sizes and confidence intervals. We can get these for the test by specifying that we want a (non-parametric) confidence interval.</p>
<pre class="r"><code>wilcox.test(dat_chess, mu = 75,
            alternative = &quot;greater&quot;, conf.int = TRUE)</code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  dat_chess
## V = 12200, p-value = 0.004361
## alternative hypothesis: true location is greater than 75
## 95 percent confidence interval:
##  76.95927      Inf
## sample estimates:
## (pseudo)median 
##        80.2828</code></pre>
<p>Interestingly, we get an infinite value for the upper bound of the confidence interval. That’s because we conduct a one sided test as we only compare if our sample average is greater. To get a confidence interval on both sides, we need a two sided test.</p>
<pre class="r"><code>wilcox_results &lt;- wilcox.test(dat_chess, mu = 75, 
                              alternative = &quot;two.sided&quot;, conf.int = TRUE)

wilcox_results</code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  dat_chess
## V = 12200, p-value = 0.008722
## alternative hypothesis: true location is not equal to 75
## 95 percent confidence interval:
##  76.30875 84.47312
## sample estimates:
## (pseudo)median 
##        80.2828</code></pre>
<p>Looks good, the 95% CI does not include my average play time of 75 minutes.</p>
</div>
<div id="the-other-black-box" class="section level2">
<h2>The other black box</h2>
<p>Here’s the proof that the Wilcoxon signed-rank test is a one sample t-test with sign-ranked transformed data:</p>
<pre class="r"><code>dat_chess %&gt;% 
  signed_rank() %&gt;% 
  t.test(mu = 75)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  .
## t = 6.2306, df = 199, p-value = 2.717e-09
## alternative hypothesis: true mean is not equal to 75
## 95 percent confidence interval:
##   92.42942 108.57058
## sample estimates:
## mean of x 
##     100.5</code></pre>
<p>Huh, what happened here? We get different values for the confidence intervals and the estimate. But that’s just because the t-test estimates the mean, while the Wilcoxon test estimates the median. The median is a bit more robust to outliers.</p>
</div>
<div id="the-regression" class="section level2">
<h2>The regression</h2>
<p>We realized that the Wilcoxon test estimates the median, and that is a bit harder with frequentist regression. But here’s the regression formula for the mean:</p>
<pre class="r"><code>ols_result &lt;- lm(signed_rank(dat_chess) ~ 1)

ols_result</code></pre>
<pre><code>## 
## Call:
## lm(formula = signed_rank(dat_chess) ~ 1)
## 
## Coefficients:
## (Intercept)  
##       100.5</code></pre>
<pre class="r"><code>confint(ols_result)</code></pre>
<pre><code>##                2.5 %   97.5 %
## (Intercept) 92.42942 108.5706</code></pre>
<p>This is the same as for the t-test on sign-ranked data.</p>
</div>
</div>
<div id="bayesian-estimation" class="section level1">
<h1>Bayesian estimation</h1>
<div id="the-model" class="section level2">
<h2>The model</h2>
<p>We could already see that we get into weird terrain with these parametric and non-parametric stuff. But reverend Bayes comes to the rescue. We can model the whole posterior distribution for our sample and can even verify the null hypothesis, not only reject it. And we are not bound to arbitrary thresholds, but can base our decision on the whole set of information, instead of point estimates. As I am playing chess from time to time, I know that most games are quite short but that some games tend to be extremely long. I have written the stackexchange post mentioned above and therefore have prior knowledge that the duration of chess games is log-normal distributed. We can use this information in the Bayesian model:</p>
<pre class="r"><code>library(brms)</code></pre>
<pre><code>## Lade nötiges Paket: Rcpp</code></pre>
<pre><code>## Loading &#39;brms&#39; package (version 2.15.0). Useful instructions
## can be found by typing help(&#39;brms&#39;). A more detailed introduction
## to the package is available through vignette(&#39;brms_overview&#39;).</code></pre>
<pre><code>## 
## Attache Paket: &#39;brms&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     ar</code></pre>
<pre class="r"><code>options(mc.cores = parallel::detectCores())  # Use all cores


bayes_result &lt;- brm(
  bf(dat_chess ~ 1), 
  family = &quot;lognormal&quot;,
  prior = c(set_prior(&quot;normal(10, 10)&quot;, class = &quot;Intercept&quot;)),
  chains = 3, iter = 2000, warmup = 1000, data = tibble(dat_chess = dat_chess))</code></pre>
<pre><code>## Compiling Stan program...</code></pre>
<pre><code>## Start sampling</code></pre>
<p>The model returns the posterior on the log-normal scale, so we should take the exponential for all values of the posterior later on. Note that I have set a rather non-informative prior on the mean. Let’s take a look at the posterior.</p>
<pre class="r"><code>result_posterior &lt;- posterior_samples(bayes_result) 

result_posterior %&gt;% 
  mutate(across(b_Intercept:sigma, exp)) %&gt;% 
  ggplot(aes(b_Intercept)) +
  geom_density(colour = &quot;grey20&quot;, fill = &quot;coral2&quot;, alpha = 0.8) +
  labs(x = &quot;Mean height&quot;, y = NULL) + 
  theme_minimal() </code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/posterior%20brms-1.png" width="576" /></p>
<p>And we can proceed to test our hypothesis:</p>
<pre class="r"><code>hyp_test &lt;- result_posterior %&gt;% 
  mutate(b_Intercept= exp(b_Intercept)) %&gt;% 
  hypothesis(&quot;b_Intercept &gt; 75&quot;)

hyp_test </code></pre>
<pre><code>## Hypothesis Tests for class :
##               Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
## 1 (b_Intercept)-(75) &gt; 0     3.56      2.09     0.23     7.06      25.09
##   Post.Prob Star
## 1      0.96    *
## ---
## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
<p>It’s 25.1 times more likely that the average chess game from the sample takes more than 75 minutes, than that the average is less than 75 minutes.</p>
</div>
</div>
<div id="comparison" class="section level1">
<h1>Comparison</h1>
<p>As we simulated the data by ourselves, we can compare each method to the true value. We used a value of 4.37 for the log-normal mean, and 0.45 for the log-normal standard deviation. We need to transform the values for each method to the log-normal scale for a proper comparison.</p>
<pre class="r"><code>dat_comparison &lt;- tibble(method = c(&quot;OLS&quot;, 
                  &quot;Wilcoxon&quot;, 
                  &quot;Bayes&quot;), 
       mean = c(ols_result$coefficients[1] %&gt;% log(), 
                wilcox_results$estimate[1] %&gt;% log(), 
                fixef(bayes_result)[1]), 
       mean_lower = c(confint(ols_result)[1]%&gt;% log(), 
                      wilcox_results$conf.int[1]%&gt;% log(),
                      fixef(bayes_result)[3]),
       mean_upper = c(confint(ols_result)[2]%&gt;% log(), 
                      wilcox_results$conf.int[2]%&gt;% log(),
                      fixef(bayes_result)[4]),
       sd = c(NA,
              NA, 
              posterior_summary(bayes_result)[2, 1]), 
       sd_lower = c(NA,
                    NA,
                    posterior_summary(bayes_result)[2, 3]),
       sd_upper = c(NA,
                    NA,
                    posterior_summary(bayes_result)[2, 4]))
       

dat_comparison %&gt;% 
  knitr::kable(digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">method</th>
<th align="right">mean</th>
<th align="right">mean_lower</th>
<th align="right">mean_upper</th>
<th align="right">sd</th>
<th align="right">sd_lower</th>
<th align="right">sd_upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">4.610</td>
<td align="right">4.526</td>
<td align="right">4.687</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Wilcoxon</td>
<td align="right">4.386</td>
<td align="right">4.335</td>
<td align="right">4.436</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="odd">
<td align="left">Bayes</td>
<td align="right">4.364</td>
<td align="right">4.313</td>
<td align="right">4.417</td>
<td align="right">0.378</td>
<td align="right">0.345</td>
<td align="right">0.418</td>
</tr>
</tbody>
</table>
<p>We can see that we get a whole posterior on sigma as well with Bayesian estimation. Let’s visualize the comparison:</p>
<pre class="r"><code>dat_comparison %&gt;% 
  ggplot(aes(mean, method, 
             xmin = mean_lower, 
             xmax = mean_upper)) +
  geom_vline(xintercept = 4.37, 
             size = 2, colour = &quot;coral2&quot;) +
  geom_pointrange(size = 0.9) +
  labs(x = &quot;Log of the mean duration of chess games&quot;, 
       y = NULL) +
  theme_minimal()</code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/unnamed-chunk-1-1.png" width="576" /></p>
<p>We can transform that to the normal scale by taking the exponential:</p>
<pre class="r"><code>dat_comparison %&gt;% 
  mutate(across(mean:mean_upper, exp)) %&gt;% 
  ggplot(aes(mean, method, 
             xmin = mean_lower, 
             xmax = mean_upper)) +
  geom_vline(xintercept = exp(4.37), 
             size = 2, colour = &quot;coral2&quot;) +
  geom_pointrange(size = 0.9) +
  labs(x = &quot;Log of the mean duration of chess games&quot;, 
       y = NULL) +
  theme_minimal()</code></pre>
<p><img src="/post/wilcoxon_signed_rank_files/figure-html/standard%20deviantion-1.png" width="576" /></p>
<p>The Bayesian regression performs best, however the Wilcoxon test still is close to the true value. This shows that when all assumptions are met, the Wilcoxon test can still be a powerfull tool. Just don’t use it without knowing what’s going on. We can see that an ordinary least squares regression (OLS) is quite far off, because it models the mean and not the median. It is therefore less robust. And I would assume that the Wilcoxon test in R performs some extra steps and corrections that leads to the better estimate for our data.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>The Wilcoxon test performs quite well but is a black box and should not be used. Of cause, if all assumptions are met, it is still fine to use this statistical test. But using Bayesian regression does not only result in better estimation, it gives us all information needed for a valid hypothesis testing and taking decisions with minimizing the expected loss.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.4 (2021-02-15)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Linux Mint 20.1
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
## 
## locale:
##  [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8    
##  [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8   
##  [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] brms_2.15.0     Rcpp_1.0.6      forcats_0.5.0   stringr_1.4.0  
##  [5] dplyr_1.0.3     purrr_0.3.4     readr_1.4.0     tidyr_1.1.2    
##  [9] tibble_3.0.5    ggplot2_3.3.3   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] minqa_1.2.4          colorspace_2.0-0     ellipsis_0.3.1      
##   [4] ggridges_0.5.3       rsconnect_0.8.16     markdown_1.1        
##   [7] base64enc_0.1-3      fs_1.5.0             rstudioapi_0.13     
##  [10] rstan_2.21.2         farver_2.0.3         DT_0.17             
##  [13] fansi_0.4.2          mvtnorm_1.1-1        lubridate_1.7.9.2   
##  [16] xml2_1.3.2           codetools_0.2-18     bridgesampling_1.0-0
##  [19] splines_4.0.4        knitr_1.30           shinythemes_1.2.0   
##  [22] bayesplot_1.8.0      projpred_2.0.2       jsonlite_1.7.2      
##  [25] nloptr_1.2.2.2       broom_0.7.3          dbplyr_2.0.0        
##  [28] shiny_1.6.0          compiler_4.0.4       httr_1.4.2          
##  [31] backports_1.2.1      assertthat_0.2.1     Matrix_1.3-2        
##  [34] fastmap_1.1.0        cli_2.2.0            later_1.1.0.1       
##  [37] prettyunits_1.1.1    htmltools_0.5.1.1    tools_4.0.4         
##  [40] igraph_1.2.6         coda_0.19-4          gtable_0.3.0        
##  [43] glue_1.4.2           reshape2_1.4.4       V8_3.4.0            
##  [46] cellranger_1.1.0     vctrs_0.3.6          nlme_3.1-152        
##  [49] blogdown_1.1         crosstalk_1.1.1      xfun_0.20           
##  [52] ps_1.5.0             lme4_1.1-26          rvest_0.3.6         
##  [55] mime_0.9             miniUI_0.1.1.1       lifecycle_0.2.0     
##  [58] gtools_3.8.2         statmod_1.4.35       MASS_7.3-53.1       
##  [61] zoo_1.8-8            scales_1.1.1         colourpicker_1.1.0  
##  [64] hms_1.0.0            promises_1.1.1       Brobdingnag_1.2-6   
##  [67] parallel_4.0.4       inline_0.3.17        shinystan_2.5.0     
##  [70] curl_4.3             gamm4_0.2-6          yaml_2.2.1          
##  [73] gridExtra_2.3        StanHeaders_2.21.0-7 loo_2.4.1           
##  [76] stringi_1.5.3        highr_0.8            dygraphs_1.1.1.6    
##  [79] pkgbuild_1.2.0       boot_1.3-27          rlang_0.4.10        
##  [82] pkgconfig_2.0.3      matrixStats_0.57.0   evaluate_0.14       
##  [85] lattice_0.20-41      rstantools_2.1.1     htmlwidgets_1.5.3   
##  [88] labeling_0.4.2       processx_3.4.5       tidyselect_1.1.0    
##  [91] plyr_1.8.6           magrittr_2.0.1       bookdown_0.21       
##  [94] R6_2.5.0             generics_0.1.0       DBI_1.1.1           
##  [97] pillar_1.4.7         haven_2.3.1          withr_2.4.1         
## [100] mgcv_1.8-33          xts_0.12.1           abind_1.4-5         
## [103] modelr_0.1.8         crayon_1.3.4         rmarkdown_2.6       
## [106] grid_4.0.4           readxl_1.3.1         callr_3.5.1         
## [109] threejs_0.3.3        reprex_1.0.0         digest_0.6.27       
## [112] xtable_1.8-4         httpuv_1.5.5         RcppParallel_5.0.2  
## [115] stats4_4.0.4         munsell_0.5.0        shinyjs_2.0.0</code></pre>
</div>
